\begin{figure}
  \centering
  \includestandalone[width=.5\textwidth]{model_diagram}
  \caption{Model Architecture}  \label{fig:model}
\end{figure}

\subsection{Naive Baseline}

In our most na√Øve model, we begin by processing the financial headlines into TF-IDF vectors.
Given the multi-level nature of the aspects, we design a hierarchical logistic regression model.
We use a local classifier per parent node approach described by~\cite{Silla:2011tp}. For the sentiment task, we fit a linear regression model on the TF-IDF vectors.

\subsection{Pre-Trained BERT}

We then experiment by fitting a pre-trained BERT model on the FIQA task.
We use the 12-layer BERT\textsubscript{BASE} base model described in~\cite{DBLP:journals/corr/abs-1810-04805}.
Discarding the pooling layer outputs, we take the outputs of the last hidden state and place a single dropout layer and dense layer on top.
We then place a linear regression sentiment head and a hierarchical aspect classification head on top.
See figure~\ref{fig:model} for the full model architecture.
We fit the model using an Adam optimizer and train both heads together.
We use categorical cross entropy loss for the aspects, and mean squared error for the sentiments, taking the sum as the total loss.

\subsection{Domain-Adapted DistilBERT}

For our third model, we adapt the neural baseline to the financial domain.
To do this, we use a large dataset of publicly available financial news articles prepared by~\cite{ding2014using}.
The dataset includes 109,110 news articles from Reuters and 450,341 from Bloomberg.
The headlines span October 2006 to November 2013.

After training a tokenizer on this dataset, we normalize the data.
Normalization steps include converting to lowercase, removing control characters, and applying normalization Form D (NFD) Unicode normalization.
We pre-tokenize on white space and punctuation.
Using a vocab size of 25,000, we train the tokenizer using the WordPiece sub-word segmentation algorithm~\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}.

Our tokenizer splits the financial news article dataset into documents of 256 tokens long.
Beginning with a pre-trained DistilBERT model~\citep{DBLP:journals/corr/abs-1910-01108}, we fine-tune our model on the tokenized Reuters dataset using a masked-language modeling (MLM) objective.
We train for 6 epochs.

We follow a similar architecture to our pre-trained BERT model, replacing the BERT model with our finance domain-adapted DistilBERT model.