%% (Domain)

Within the field of behavioral finance, there are two broad types of sentiment that are analyzed: investor sentiment and textual sentiment. Investor sentiment is a belief about future cash flows and investment risks that is not justified by the facts at hand \citep{BakerMalcolm2007ISit}. As a survey-based approach, it attempts to capture the subjective judgements of individual investors. Textual sentiment refers to the degree of positivity or negativity in financial texts. This approach attempts to extract information about investors’ moods from corporate disclosures/filings, media articles, internet messages, and other corpora. Textual sentiment captures both subjective judgements and objective conditions in financial markets.

%While textual sentiment can include aspects like active-passive and strong-weak, the majority of interest is in positivity-negativity.

%%## Motivation

While there are some areas of disagreement in the existing literature on the relationship between textual sentiment and financial markets, the existing literature largely agrees that textual sentiment has potentially strong impacts on stock returns, trading volumes, volatility, and abnormal returns \citep{KearneyColm2014Tsif}. Negative sentiment exerts downward pressure on market prices, increased trading volume, and weakly predicts market volatility \citep{TetlockPaul2007GCtI}. This effect is particularly pronounced during recessions \citep{garciadiego2013SdR} and for smaller companies \citep{FergusonNickyJ2015MCaS}. While negative media coverage has a larger effect than positive, during corporate acquisitions, positive media coverage is predictive of acquisition success \citep{buehlmaier2015role}.

Textual sentiment can also provide clues for trading around market reversals. Sentiment can predict short-term reversals, temporary increases in volatility, and a shift of capital from riskier stocks into safer bonds \citep{DaZhi2015TSoA}. And when sentiment and market returns point in the same direction, the market is likely overreacting and prone to a price reversal \citep{froot2017media}. While financial markets are quick to price in inefficiencies, improved measures of textual sentiment hold the promise of providing outsized returns to early movers. If we can better measure and model textual sentiment, we will have an advantage over other market participants.


%While negative media coverage has a larger effect than positive, during corporate aquisitions, positive media coverage is predictive of acquisition success. % Buehlmaier (2013)


%News stories about earnings have information, hard to process, long term fundamentals \citep{Engelberg2008CostlyIP}% Engelberg (2008)

% Tetlock et al. (2008)

% Sinha (2010)

% Carretta et al. (2011)

% Engelberg et al. (2012)

% Ferguson et al. (2012)


% Liu and McConnell (2013)





%Portfolio of negative sentiment stocks that have performed poorly outperform high-flyers with positive sentiment \cite{froot2017media}. Buy losers with low sentiment. Market overreact when sentiment and price action moving together.% DEF INCLUDE BC NEW Media reinforcement in international financial markets Kenneth Froot (2017)

% NEW Are financial constraints priced? Evidence from textual analysis (2018)
%constraint - trouble issuing debt or equity. smaller companies.
%\cite{BuehlmaierMatthiasMM2018AFCP}

%Textual sentiment in media articles conveys additional information not captured by traditional quantitative financial information such as financial statements.







%% Problem Statement

Sentiment analysis is a sub-field of natural language processing (NLP). It aims to measure sentiment in text, audio, and other media. Aspect-Based Sentiment Analysis (ABSA) is a branch of sentiment analysis which deals with both extracting the opinion targets (aspects) and the sentiment expressed towards them. Given a collection of documents, the first subtask of ABSA is to identify all aspects present in the document. For example, the news headline “How Kraft-Heinz Merger Came Together in Speedy 10 Weeks” is about the entity ‘Kraft-Heinz.’ The aspect of Kraft-Heinz discussed is 'corporate M\&A activity'. For the second subtask, we assume the aspect terms are given. The task is to identify the sentiment expressed towards the aspect. In the example above, annotators labeled the headline as having slightly positive sentiment (0.214) because the deal was successful and came together quickly. Thus, ABSA can be formulated as both a classification and regression task.


%% Existing Methods Fall Short (I could actually cut this)

%Traditional methods of measuring sentiment in the financial domain restrict
%power of representations...

%The major limitation of ...





% Introduce FIQA

In this article, we investigate different methods of measuring aspect-based sentiment within the financial domain.  We begin with a pre-trained BERT language model. Then we fine-tune it on an unlabeled dataset of 559,451 financial headlines from Bloomberg and Reuters. We evaluate our approach on sub-task 1 of the FiQA 2018 challenge. Because of the small size of the FiQA dataset, we also experiment with a semi-automated approach to label data.

%We investigate several methods of reading a financial headline, identifying aspects, and identifying the assocaited polarity.


%We then show...
% I really like the contributions section of the 'Financial Aspect-Based Sentiment Analysis using Deep Representations' paper. Really, I like almost all of that paper.

The following are the primary contributions of this paper:

1. We assess the performance of recent NLP techniques such as ...... on the FIQA dataset. (which has a limited number of training examples)

% The rest of the paper is strucured as follows.

\section{Literature Review}

We separate our related work into two areas: first, recent advancements in neural language processing. Second, deep learning methods applied to aspect-based sentiment analysis.

\subsection{Recent Advances in Natural Language Processing}



%During the last decade...

Our work broadly falls under the category of semi-supervised sequence learning for natural language. There are four recent trends to pay attention to: deep contextual embeddings, pre-trained language models, a reduction in task-specific architectures, and task- and domain-specific fine-tuning.

Because of their ability to transform words from a discrete representation to a continuous one, Word2vec \citep{mikolov2013efficient} was a standard component in many NLP systems. However, their context-independent nature limited their usefulness. For example, an embedding for the word ‘play’ is identical when used to describe a sporting event as for actors putting on a dramatic work.
% Maybe also add a comment on how the method is linear (shallow), not deep.

One of the first methods to encode context in words was proposed by \cite{dai2015semisupervised}. Using unlabeled data, they pre-trained a conventional language model using a next-word prediction objective. After fine-tuning the model on multiple sentiment classification tasks, they achieved best in class results. The method had limitations, though. By using a long short-term memory (LSTM) architecture, gradients became unstable for long documents.

\cite{peters2018deep} built upon this approach. Using a similar pre-training objective, they pre-train a large language model on a dataset of 30 million sentences. However, they did not fine-tune the language mode. Instead they used the resulting model as a feature-extractor for task-specific architectures. After pre-training, they feed sentences into the language model. The output is a deep-contextual character embedding. They now discard the language model. Next, the character-level features are passed into a task-specific architecture. This design allowed them to achieve state-of-the-art results on an array of natural language tasks (SQuAD, SNLI, SRL, Coref, NER, SST-5) by solely swapping out Word2vec embeddings for ELMo embeddings.

It wasn’t until GPT-1 where task-specific architectures begin to be discarded. Using a transformer architecture, \cite{radford2018improving} pre-trained a language model. Their use of transformers allowed the model to learn longer range dependencies in the text. They then use the entire pre-trained model as a base for supervised models. They showed that a pre-trained model, with little adaptation, can transfer learned representations to a multitude of NLP tasks. GPT-1 improved state-of-the art across 9 of 12 NLP benchmarks. These tasks covered 4 broad categories: natural language inference, question answering, semantic similarity, and text classification. However, the language model struggled on tasks such as part-of-speech tagging. This is because of the left-to-right masking and next-word-prediction pre-training objective. It did solidify the approach of using task-agnostic architecture in contrast to ELMo’s feature extraction method.

\cite{DBLP:journals/corr/abs-1810-04805} addresses GPT-1’s left-to-right limitation by introducing a new pre-training objective. Instead of predicting the next word, they train their BERT model on with a Masked Language Modeling (MLM) objective. This allows the language model to benefit from both left-context, right-context, or a combination of both. They also train the model on the largest dataset to date. By combining bidirectional pre-training and an internet-scale dataset, they achieved state-of-the-art on 11 NLP tasks. This was the strongest demonstration to date that language models are compute-bound. BERT combines the best of previous approaches: a large pre-trained language model that learned deep contextual embeddings, and fine-tuning a general purpose model.











%In one of the early methods to...

%One of the early papers to leverage semi-supervised sequence learning

%Big trends
%- Deep contextual embeddings
%- Pre-trained language models
%- Reduction in task-specific architectures
%- Task and domain specific fine-tuning.




%Over the last few years, researchers have ...(demonstrated?)

%Recent approaches have investigated...

\subsection{Deep Learning Approaches to Aspect-Based Sentiment Analysis}

%... is a special case of ...

%Early works ...

%In recent work ...

%Research into aspect based sentiment analysis .... following methodolgies (approaches).

%The datasets typically used for Aspect-Based Sentiment Analysis are ...


%TODO - make sure I have a book reference
%TODO - make sure I have an arxiv reference

%(Early work) Early works on ABSA ....

%(Recent DL improvements) Recent advances in deep contextual embeddings.

%Trend towards pre-trained models. with fine-tuning.

%Since the early works on ABSA [18], [19], [20], several methods have been put forward to address the problem. In this section, we review some of the works which have utilized deep learning techniques.

%(How recent leverages) Recent methods leverage this approach.

